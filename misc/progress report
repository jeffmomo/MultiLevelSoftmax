## Things we can implement ##

Have the classifier gain the ability to train records which do not have the actual label, but only a label at a certain point
in the hierarchy. 

e.g. a rare spider species which only have 3 images dedicated to it. We dont discard it, 
and train it across the entire spider family.
-----
----
-----
Randomised ReLU?
-----
Produce actual probabilities?

Maybe produce all "probabilities" for all leaf nodes, sum them, and use current "probability" divide by that.
this is probably not possible
---

Weighted for rarer classes?

---
Include images which are in a class with few examples, labling them only with those in the hierarchy which have
enough numbers in them. 

---
Find a way to filter out irrelevant images, e.g. for the bear class, take out any bear bones etc. 
We could do this by running it through a convolutional autoencoder and take out the images which do not do well.
In an ideal situation, the neural network should be able to recognise based upon these images, however due to the 
relatively few number of them, this is impractical. 

Following on,
We could develop a reinforcement learning system in which the network decides if an image is worth training on or not. 
This can be done given a measure of how 'ready' a network is (i.e. its capability of making good decisions, can use avg. network accuracy)
and, on a sample piece of data, how different it is from the network's prediction. We use these 2 measures to produce a "weight" which is then
used on the sample. 
Under this mechanis, we can see the network initially accepting all the noisy inputs with a high weight, only later on when it can 
confidently distinguish outliers will it decide to lower its weight.
----

Top-k dijkstra

----
Unsupervised partitioning of training data, such that they belong to distinct groups and can be trained on

----

### Musings ###

Chainging to using transfer learning through fine-tuning the Imagenet Inceptionv3 network.
I am doing this because the time to train 1 million images on a state-of-the-art network will take too long.
Caveats: Not sure if we can use a different loss function on top. Maybe we can using normalised probabilities.

No way to give influence to the size of the species in question. Some species are different due only to its size, making it difficult to disambiguate


Having ALL nodes at the hierarchy being a class in the neural network.
Have a uniform prior, e.g. [0, 0.25, 0.25, 0, 0.25, 0.25, 0] with all the labels being non-zero but equal to each other.
At the classification stage, for each level in hierarchy, find the highest prob. for nodes in that level under the prior node.
Descent to bottom.



Deliberate training with mislabeled data - may allow the network to learn to better disambiguate between them
---

## Progress reports ##


-----ongoing-----



Display bounding boxes

Web app allow specification of prior

Apply label splitting

Apply unsupervised bounding box generation

Add web application which allows scaling, cropping, rotating, and also allows uploading from URL

need to put in report the class distribution and number of samples per class.

Changed inception_eval and image_processing such that the original labels are also included with the embeddings

Implement histogram thingy which shows the accuracy of each class on each layer in a graphical form

Get images using google images by searching the species, and use those as a new, high quality dataset to evaluate on. 

Disable label smoothing

automate label generation and TFRecord generation

Redo input augmentation for inceptionv3 as it is too much cropping - Refinement now

tensorflow using the preprocessed target vector instead of purely labels
Do this using tf.constant, and tf.gather.

Testing transfer learning on bigger data set



-----done----

Find out way to produce top-k most likely labels.

Adding hierarchy chain to evaluation outputs (needs testing) - hierarchical_eval.py

Apply label spillage. Too slow to train.

Using Inception Resnet

Build web classifier

Using Adam

Removed random hues.

Building 2090 class dataset.

build node service which shows misclassified images

Insetad of using average, now uses the sum of the normalised probabilities to descend down layers. More mathematically sound.

"""Normalising significance against counts to account for class imbalances ? ? ?
for example if under the animal parent there is a cat and a dog, and there are 1 cat img and 1000 dog images,
the cat image will be significantly biased towards, as it will have 1000 * k trained against it, while dog will only have 1 * k."""

Tensorflow transfer learning

Tensorflow input augemntation

Normalise target vectors to a unit length of 1

Prune tree such that only nodes with a certain number of observations remain

Can transform string-based hierarchy chains into index-based hierarchy chains for use in the Layer structure

Can begin classification from an arbitrary level of hierarchy.

Can generate target training vectors using the structure with hierarchical hints

Able to use that hierarchy to build structured layers which we can hierarchically evaluate off

Finished creating hierarchy from observations

Grabbed 1.2M images so far

Created python file which multi-treadedly grabs image files off the specified link
However, some files are massive, meaning we cannot store the million observations without taking up too much space.

Thus we need to apply the transformations to each image as we download them.
This will be done using OpenCV.

We will only take the central 50% of the pixels into consideration, and we have to normalise all images such that they have 
an equal size and aspect ratio.

### How To Use ###

1. Generate taxonomy tree with the hierarchy chains
2. Get the leaf node (or not, if you want to use Genus) definitions as a list (this will be the labels vector)
2a. If necessary, prune the tree with a count threshold
3. Build using this definitions list
4. We can now generate target vectors using a name-base hierarchy chain, given a significance factor
4. Generate a Layer and a mapping from fullname to layer index
5. We can now the mapping to map from a name-based hierarchy to a index-based hierarchy for use with the Layer
6. Use the Layer (possibly along with a priors chain) to evaluate an output vector

